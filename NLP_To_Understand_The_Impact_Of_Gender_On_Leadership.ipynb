{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Objective:**  \n",
    "Identify how leaders inspire graduates and what role gender plays in their approach.\n",
    "\n",
    "**Approach:**  \n",
    "Use Natural Language Processessing (NLP) to explore NPR's top 350 commencement speeches. Identify themes and patterns using exploratory data analysis, topic modeling, and sentiment analysis. \n",
    "\n",
    "**Data:** \n",
    "* Speeker name, school, and year scraped from NPR's top 350 commencement speeches\n",
    "* Speech transcripts were scraped from YouTube using custom selenium script\n",
    "\n",
    "**Results Summary:**  \n",
    "Top topics: Politics, Career, Hope, Culture, Education\n",
    "* Male distrobution: Hope, Politics, Career, Education, Culture\n",
    "* Female distrobution: Hope, Education, Culture, Career, Politics\n",
    "\n",
    "Sentiment is generally positive but reflects an upside down bell curve. Both males and females begin and end on strong positive notes and use the middle to speak to passion points. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import pickle\n",
    "import re\n",
    "import string\n",
    "import logging\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import  TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.datasets import make_blobs, fetch_mldata\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, RegexpTokenizer\n",
    "from nltk.util import ngrams\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import scipy.sparse\n",
    "\n",
    "from ibm_watson import ToneAnalyzerV3\n",
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Scraping and Storing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Beautiful Soup Scraping Top Speeches Form NPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def load_page_for_scraping():\n",
    "    \"\"\"Requests connection to the NPR page and prints confirmation if the page is\n",
    "    loaded correctly. If fails to load, reports error. Returns beautiful soup\n",
    "    object for scraping.\"\"\"\n",
    "\n",
    "    print(\"Connecting to NPR\")\n",
    "    url = 'http://apps.npr.org/commencement/'\n",
    "    response = requests.get(url)\n",
    "    status = response.status_code\n",
    "    if status == 200:\n",
    "        print(\"Page loaded succcessfully\")\n",
    "        page = response.text\n",
    "        soup = BeautifulSoup(page, \"lxml\")\n",
    "        return soup\n",
    "    else:\n",
    "        print(f\"Error code: {status}\")\n",
    "        return\n",
    "\n",
    "def scrape_NPR():\n",
    "    \"\"\"For NPRs 350 top commencement speeches, scrapes the speeker name, school name,\n",
    "    and year speech was given. Returns list of speech information.\"\"\"\n",
    "    soup = load_page_for_scraping()\n",
    "    print(\"Scraping speeker name, school, and year.\")\n",
    "\n",
    "    speeches = []\n",
    "    speech_names = soup.find_all('h2', class_='speech-name')\n",
    "    speech_schools = soup.find_all('p', class_='speech-school')\n",
    "    speech_years = soup.find_all('p', class_='speech-year')\n",
    "\n",
    "    for i in range(len(speech_names)):\n",
    "        speeches.append([speech_names[i].text, speech_schools[i].text, speech_years[i].text])\n",
    "\n",
    "    print(\"Speeker names, school, and year scraped.\")\n",
    "\n",
    "    return speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "speeches = scrape_NPR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "speeches[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Selenium Scraping Video Transcripts From YouTube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def connecting_YouTube():\n",
    "    \"\"\"Connects to YouTube and confirms scraping permissions.\"\"\"\n",
    "    print(\"Confirming scraping allowance on YouTube.\")\n",
    "    url = 'https://www.youtube.com/robots.txt'\n",
    "    response  = requests.get(url)\n",
    "    print(response.text)\n",
    "    return\n",
    "\n",
    "connecting_YouTube()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# track transcripts that have been scraped and transcripts that remain in case process stops\n",
    "# part way through\n",
    "\n",
    "all_scraped_content = []\n",
    "remaining = [speech[0] for speech in speeches]\n",
    "i = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def gettranscript(speech, i):\n",
    "    \"\"\"loads a selenium driver with youtube search result for the speech provided.\n",
    "    Scrapes the transcript from that youtube page and returns a list of the speech\n",
    "    information with the transcript.\"\"\"\n",
    "\n",
    "    waittime = 10\n",
    "    sleeptime = [5,15]\n",
    "\n",
    "    # connect to selenium driver\n",
    "    chromedriver = \"/Applications/chromedriver\"\n",
    "    os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "\n",
    "    driver = webdriver.Chrome(chromedriver)\n",
    "\n",
    "    sleep(random.uniform(sleeptime[0],sleeptime[1]))\n",
    "    \n",
    "    # set wait conditions for optimal scraper performance\n",
    "    wait = WebDriverWait(driver, 3)\n",
    "    presence = EC.presence_of_element_located\n",
    "    visible = EC.visibility_of_element_located\n",
    "\n",
    "    query = speech[0] + ' ' + speech[1] + ' ' + speech[2] + ' ' + 'commencement speech'\n",
    "    driver.get(\"https://www.youtube.com/results?search_query=\" + str(query))\n",
    "\n",
    "    wait.until(visible((By.ID, \"video-title\")))\n",
    "    driver.find_element_by_id(\"video-title\").click()\n",
    "    sleep(random.uniform(2,4))\n",
    "    \n",
    "    # try and except sequence to continue process if a video / transcript does not exist\n",
    "    try:\n",
    "        element = driver.find_element_by_xpath('//button[@aria-label=\"More actions\"]')\n",
    "    except:\n",
    "        msg = 'could not find options button'\n",
    "        driver.quit()\n",
    "        print(msg)\n",
    "        return msg\n",
    "\n",
    "    try:\n",
    "        sleep(random.uniform(2,5))\n",
    "        element.click()\n",
    "    except:\n",
    "        msg = 'could not click'\n",
    "        driver.quit()\n",
    "        print(msg)\n",
    "        return msg\n",
    "\n",
    "    try:\n",
    "        element = driver.find_element_by_xpath('//ytd-menu-service-item-renderer[@aria-selected=\"false\"]')\n",
    "    except:\n",
    "        msg = 'could not find transcript in options menu'\n",
    "        driver.quit()\n",
    "        print(msg)\n",
    "        return msg\n",
    "\n",
    "    try:\n",
    "        sleep(random.uniform(2,5))\n",
    "        element.click()\n",
    "    except:\n",
    "        msg = 'could not click'\n",
    "        driver.quit()\n",
    "        print(msg)\n",
    "        return msg\n",
    "\n",
    "    try:\n",
    "        sleep(random.uniform(2,4))\n",
    "        element = driver.find_element_by_xpath('//ytd-transcript-body-renderer[contains(@class, \"style-scope\")]')\n",
    "    except:\n",
    "        msg = 'could not find transcript text'\n",
    "        driver.quit()\n",
    "        print(msg)\n",
    "        return msg\n",
    "\n",
    "    tscript = element.text\n",
    "    \n",
    "    #append transcript to list along with the speaker name, school, and speech year\n",
    "    body_text_ls = [speech[0], speech[1], speech[2], tscript]\n",
    "\n",
    "    driver.quit()\n",
    "    print(f'Speech number {i}, {speech[0]} scraped')\n",
    "\n",
    "    return body_text_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# iterate through list of speeches obtained from NPR scraping\n",
    "# look up each speech on YouTube and scrape the transcript\n",
    "\n",
    "for speech in speeches:\n",
    "    scraped_content = gettranscript(speech, i)\n",
    "    all_scraped_content.append(scraped_content)\n",
    "    i += 1\n",
    "    remaining.pop(0)\n",
    "    \n",
    "# Note: if process above breaks, re-run with only the speeches in the remaining list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Store all scraped content in pickle to prevent losing data if system errors\n",
    "with open('scraped_content.pkl', 'wb') as f:\n",
    "    pickle.dump(all_scraped_content, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Uploading to MongoDB Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# For speeches that did not have transcripts on YouTube, they were manually added to a list pickled\n",
    "# in a different document. Read in manual list.\n",
    "\n",
    "manually_added_speeches = pickle.load( open( \"manual_speeches.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def upload_to_mongo(scraped_list):\n",
    "    \"\"\"conneccts to mongo database 'speeches'. Uploads raw content from commencement\n",
    "    speech web scraping into database.\"\"\"\n",
    "    \n",
    "    client = MongoClient()\n",
    "\n",
    "    # connect to database within mongoDB\n",
    "    speech_db = client.speeches\n",
    "\n",
    "    # add each speech to MongoDB\n",
    "    for speech in scraped_content:\n",
    "        speech_dict = {'name': speech[0],\n",
    "                       'school': speech[1],\n",
    "                       'year': speech[2],\n",
    "                       'speech': speech[3]}\n",
    "        speech_db.speech_collection.insert_one(speech_dict)\n",
    "\n",
    "    print(\"All speeches uploaded to Mongo.\")\n",
    "    print(\"Speech count in Mongo:\", speech_db.speech_collection.count())\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "upload_to_mongo(scraped_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "upload_to_mongo(manually_added_speeches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Loading & Cleaning Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Retrieving From MongoDB Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def pull_from_mongo():\n",
    "    client = MongoClient()\n",
    "    speech_db = client.speeches\n",
    "    speech_collection = speech_db.speech_collection\n",
    "\n",
    "    cursor = speech_collection.find()\n",
    "\n",
    "    df = pd.DataFrame(list(cursor))\n",
    "\n",
    "    return df\n",
    "\n",
    "speeches = pull_from_mongo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "speeeches.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def clean_text_round1(text):\n",
    "    \"\"\"Make text lowercase, remove text in square brackets, \n",
    "    remove punctuation and remove words containing numbers.\"\"\"\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "round1 = lambda x: clean_text_round1(x)\n",
    "speeches.speech = speeches.speech.apply(round1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def clean_text_round2(text):\n",
    "    \"\"\"Remove additional punctuation and non-sensical \n",
    "    text that was missed the first time around.\"\"\"\n",
    "    text = re.sub('[‘’“”…]', '', text)\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    return text\n",
    "\n",
    "round2 = lambda x: clean_text_round2(x)\n",
    "speeches.speech = speeches.speech.apply(round2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#drop mongodb unique ID - not needed for analysis\n",
    "speeches = speeches.drop('_id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# removing two speeches that downloaded in spanish\n",
    "spanish_speeker1 = 'Henry A. Wallace'\n",
    "spanish_speeker2 = 'Billy Collins'\n",
    "\n",
    "speeches = speeches[speeches.name != spanish_speeker1]\n",
    "speeches = speeches[speeches.name != spanish_speeker2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# A few duplicates were loaded to Mongo - drop them and keep only the first occurance\n",
    "speeches = speeches.drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#save cleaned data to csv file to access later\n",
    "speeches.to_csv('speeches_df_basic_cleaning.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Add gender column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# gender information grabbed manually and saved in pickled list\n",
    "m_f_designation = pickle.load( open( \"m_f_designation.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# remove any \\n new from string and split into a list\n",
    "gender = re.sub('\\n', ' ', m_f_designation).split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#create new column with speeker gender\n",
    "speeches[\"gender\"] = gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# confirm column was added correctly \n",
    "speeches.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Speech Length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "speeches.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Find the number of words in each document\n",
    "\n",
    "def length_of_speeches(data):\n",
    "    \"\"\"creates new column with speech length for each speech\"\"\"\n",
    "    \n",
    "    data['speech_len'] = data.speech.apply(lambda x: x.split())\n",
    "    data['speech_len'] = data.speech_len.apply(lambda x: len(x))\n",
    "    \n",
    "    return data\n",
    "\n",
    "speeches = length_of_speeches(speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# plot the distrobution\n",
    "plt.hist(speeches.speech_len, bins=50)\n",
    "plt.title('Distrobution of Speech Lengths')\n",
    "plt.xaxis('Speech Length')\n",
    "plt.yaxis('Count');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# find the longest, shortest, and average speech length\n",
    "print(\"Min\", np.min(speeches.speech_len))\n",
    "print(\"Max\", np.max(speeches.speech_len))\n",
    "print(\"Mean\", np.mean(speeches.speech_len))\n",
    "\n",
    "speeches.speech_len.describe([.03, .25, .75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Remove speeches that are less than 500 words (errors in scraping)\n",
    "speeches = speeches[speeches.speech_len > 500]\n",
    "\n",
    "speeches.speech_len.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### By Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create Male only DF\n",
    "males = speeches[speeches.gender == '0']\n",
    "\n",
    "# Create femail only DF\n",
    "female = speeches[speeches.gender == '1']\n",
    "\n",
    "print(f\"There are {len(males)} males and {len(female)} females in the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# plot a histogram of the length of speeches for males and females\n",
    "\n",
    "plt.hist(female.speech_length, bins=50, alpha=.3, label='female')\n",
    "plt.hist(males.speech_length, bins=50, alpha=.4, label='male')\n",
    "plt.xaxis('Speech Length')\n",
    "plt.yaxis('Count')\n",
    "plt.title('Length of Speeches for Males and Females')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('Average speech length for females:', np.mean(female.speech_length))\n",
    "print('Average speech length for males:', np.mean(males.speech_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Identify Common Words Overall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Create simple document-term matrix to explore common words\n",
    "def simple_doc_term():\n",
    "    cv = CountVectorizer(stop_words='english', ngram_range=(1,1))\n",
    "    data_cv = cv.fit_transform(speeches.speech)\n",
    "    data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "    data_dtm.index = speeches.index\n",
    "    return data_dtm\n",
    "\n",
    "data_dtm = simple_doc_term()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#top 20 most commonly said words\n",
    "np.sum(data_dtm, axis=0).sort_values(ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# most of these are not meaningful and will be added to stop words list\n",
    "# view the next set of 20 for more information\n",
    "np.sum(data_dtm, axis=0).sort_values(ascending=False)[20:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### By Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "female_dtm = simple_doc_term(female)\n",
    "male_dtm = simple_doc_term(males)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "top_female_words = np.sum(female_dtm, axis=0).sort_values(ascending=False)[:20]\n",
    "top_male_words = np.sum(male_dtm, axis=0).sort_values(ascending=False)[:20]\n",
    "\n",
    "print(\"Top female words:\")\n",
    "print(top_female_words)\n",
    "\n",
    "print(\"Top male words:\")\n",
    "print(top_male_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Prepare Dataframe and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create a new column in the speeches dataframe with lemmatized words\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "speeches['lemmatized_words'] = speeches.speech.apply(lambda x: x.split())\n",
    "speeches.lemmatized_words = speeches.lemmatized_words.apply(lambda x: [lem.lemmatize(y) for y in x])\n",
    "speeches.lemmatized_words = speeches.lemmatized_words.apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    \"\"\"Displays each topic and the words that fall within them.\"\"\"\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create stop words list\n",
    "def new_stopwords():\n",
    "    my_stop_words = set(['like', 'know', 'im', 'just', 'thank', 'dont', 'youre', 'get', 'would', 'said', 'thats',\n",
    "                         'think', 'say', 'things', 'us', 'going', 'way', 'really', 'well', 'many', 'got', 'right', \n",
    "                         'something','thing', 'didnt', 'wa','one','went','wanted','ha','one','lot','mean', 'want', \n",
    "                         'congratulations', 'commencement','staff','speaker','trustees','board','members', \n",
    "                         'everything','guy', 'someone', 'everyone', 'ive', 'actually', 'theyre','youll', 'come', 'dr',\n",
    "                         'anything', 'new', 'also', 'says', 'must', 'though','even', 'today', 'kind', 'hes', 'stuff',\n",
    "                         'somebody', 'gon', 'york', 'day','women','men','woman','man','lets','id','guys', 'let','tell',\n",
    "                         'cant','thought','great','look','always','cant','big','see','take','never','back','little',\n",
    "                         'need','maybe','every','still','ever','two', 'around','honor', 'three','please','called',\n",
    "                         'may', 'yeah','high', 'mr', 'better','part','good','first','show','feel','oh','else','whats',\n",
    "                         'doesnt','sure','put','getting','later','wasnt','okay','knew', 'could', 'gonna', 'every',\n",
    "                         'made', 'youve', 'much', 'theres', 'cover','none', 'acts', 'john','words','person',\n",
    "                         'without', 'old', 'kid', 'order', 'everybody','ways', 'group','point', 'applause', 'adam',\n",
    "                         'sarah','sara', 'bridge', 'finally', 'suppose', 'effect', 'excellent', 'probably','enough',\n",
    "                         'thanks','guest','speak','turn','ago','since','havent','side','week','month', 'sense',\n",
    "                         'others','days','days', 'mit', 'might', 'michael','william','david','story','place','real',\n",
    "                         'word','told','away','next','came', 'find','harvard', 'number','done','night','doe','long',\n",
    "                         'weve', 'talk','best','call','asked','another','keep','free','whether','end','wait','four',\n",
    "                         'door','become', 'orleans', 'affect','meal', 'tap','step', 'girl','room','play', 'yes', \n",
    "                         'start', 'true','last','wood','sort', 'sometimes','tony', 'michigan','sweet','small',\n",
    "                         'parent','folk','mom','dad','song','child','took', 'pas', 'across','amount',\n",
    "                         'car','eye','face','bit'])\n",
    "\n",
    "    eng_stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    new_stopwords = eng_stop_words.union(my_stop_words)\n",
    "    \n",
    "    return new_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## All Data Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Note: additional exploration was done to determine dimensionality reduction conditions. Exploration included all variations of the following. \n",
    "1. LDA, LSA, NMF \n",
    "2. CountVectorizer and TFidfVectorizer \n",
    "3. Hyperparameters including stop words, max_df, min_df, and ngram_range\n",
    "4. Lemmatization and stemming\n",
    "5. Part of speech: nouns only, verbs only, adjectives only, nouns + verbs, nouns + adjectives\n",
    "\n",
    "Winning condition used below: \n",
    "* NMF with TF-IDf\n",
    "* Lemmatization with all words\n",
    "* Stop words above\n",
    "* Max_df = .8, min_df = .3\n",
    "* Single word ngrams only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# use TF-IDF vectorizing\n",
    "vectorizer = TfidfVectorizer(stop_words=new_stopwords, max_df=.8, min_df=.3, ngram_range=(1,1))\n",
    "doc_term_object = vectorizer.fit_transform(speeches.lemmatized_words).toarray()\n",
    "\n",
    "# create NMF object and transform the document term object created above\n",
    "nmf = NMF(5, random_state=19)\n",
    "doc_topic = nmf.fit_transform(doc_term_object)\n",
    "\n",
    "#view top words in each topic\n",
    "display_topics(nmf, vectorizer.get_feature_names(), 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Based on above, topics are determined to be:\n",
    "* Career\n",
    "* Education\n",
    "* Hope\n",
    "* Culture\n",
    "* Politics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Viewing Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### General - All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#create dataframe of topic distrobutions per document to add onto the speeches dataframe\n",
    "topic_columns = ['career','politics','education','hope','culture']\n",
    "nmf_df = pd.DataFrame(doc_topic, columns=topic_columns)\n",
    "\n",
    "#concatenate origonal speeches dataframe and topics dataframe\n",
    "categorized_speeches = pd.concat((speeches.reset_index(), nmf_df),axis=1)\n",
    "\n",
    "# remove columns with speech info (no longer needed for analysis)\n",
    "categorized_speeches = categorized_speeches.drop(['lemmatized_words', 'index'], axis=1)\n",
    "\n",
    "# create a column to indicate the top topic for each document\n",
    "categorized_speeches['top_topic'] = categorized_speeches[topic_columns].idxmax(axis=1)\n",
    "\n",
    "# view top topics per document\n",
    "categorized_speeches.top_topic.value_counts()\n",
    "\n",
    "# save the dataframe to access later\n",
    "categorized_speeches.to_csv('topic_modeling_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# plot the distrobution of top topics\n",
    "x = list(categorized_speeches.top_topic.value_counts().index)\n",
    "y = list(categorized_speeches.top_topic.value_counts())\n",
    "\n",
    "chart = sns.barplot(x=x, y=y, color='#00274C')\n",
    "plt.xlabel('Top Topic')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Top Topic Distrobution')\n",
    "sns.despine();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# plot with t-sne\n",
    "\n",
    "def t_sne_prep(doc_term_object)\n",
    "    \"\"\"Takes in a document-term matrix created during topic modeling and returns a \n",
    "    t-sne dataframe with two columns for 2-dimensional graphing and a labels column\n",
    "    to indicate topic.\"\"\"\n",
    "    doc_term_df = pd.DataFrame(doc_term_object)\n",
    "\n",
    "    ss = StandardScaler()\n",
    "    doc_term_df = ss.fit_transform(nmf_df)\n",
    "\n",
    "    tsne = TSNE(n_components=2, random_state=19, verbose=1, n_iter=2000, learning_rate=10)\n",
    "    tsne_results = tsne.fit_transform(doc_term_df)\n",
    "\n",
    "    #concat pca dataframe with the top topics per document\n",
    "    t_sn_df = pd.concat((pd.DataFrame(tsne_results), categorized_speeches.top_topic), axis=1)\n",
    "    t_sn_df = t_sn_df.rename(columns={0:'first',1:'second'})\n",
    "    \n",
    "    return t_sne_df\n",
    "\n",
    "t_sne_df = t_sne_prep(doc_term_object)\n",
    "\n",
    "# plot\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "vis = sns.scatterplot(x=t_sn_df['first'], y=t_sn_df['second'], lw=0, s=40, hue=t_sn_df['top_topic'])\n",
    "plt.xlim(-25, 25)\n",
    "plt.ylim(-25, 25)\n",
    "vis.legend(loc=3)\n",
    "sns.despine();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### By Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create separate dataframes for males and females\n",
    "male_categorized_speeches = categorized_speeches[categorized_speeches.gender == '0']\n",
    "female_categorized_speeches = categorized_speeches[categorized_speeches.gender == '1']\n",
    "\n",
    "# compare percentage of each topic since the number of males > number of females\n",
    "print('Top male topics:')\n",
    "print(male_categorized_speeches.top_topic.value_counts(normalize=True))\n",
    "\n",
    "print('Top female topics:')\n",
    "print(female_categorized_speeches.top_topic.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create a dataframe with the topic distrobutions for men and women to visualize in Tableau\n",
    "topic_distro = pd.DataFrame({'men': full_m.top_topic.value_counts(normalize=True), 'women': full_f.top_topic.value_counts(normalize=True)})\n",
    "\n",
    "# export for visualization\n",
    "topic_distro.to_csv('topic_distro_gender.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Vader - By Gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Full Speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create a copy in case of needing to revert to previous state\n",
    "sent_df = categorized_speeches.copy()\n",
    "\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "# for each of the speeches, create columns for the four sentiments captured by Vader to analyze\n",
    "sent_df['pos'] = sent_df.speech.apply(lambda x: analyser.polarity_scores(x)['pos'])\n",
    "sent_df['neg'] = sent_df.speech.apply(lambda x: analyser.polarity_scores(x)['neg'])\n",
    "sent_df['neu'] = sent_df.speech.apply(lambda x: analyser.polarity_scores(x)['neu'])\n",
    "sent_df['comp'] = sent_df.speech.apply(lambda x: analyser.polarity_scores(x)['compound'])\n",
    "\n",
    "# view and compare results for each gender\n",
    "sent_df.groupby('gender')['pos','neg','neu','compount'].agg(['mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### By Sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Observe the change in sentiment throughout the speeches to identify overarching patterns and if there are any differences between the genders. To do so, speeches are broken into 10 smaller segments and each segment has sentiment analysis applied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create a copy in case of needing to revert to previous state. Maintain only gender and speech \n",
    "# columns to avoid clutter\n",
    "sent_df_split = categorized_speeches[['gender','speech']]\n",
    "\n",
    "# split speeches into 10 sections, each is a new column to keep the speech together \n",
    "sent_df_split['s1'] = sent_df_split.speech.str.split().apply(lambda x: ' '.join(x[0:int(len(x)/10)]))\n",
    "sent_df_split['s2'] = sent_df_split.speech.str.split().apply(lambda x: ' '.join(x[int(len(x)/10):2*int(len(x)/10)]))\n",
    "sent_df_split['s3'] = sent_df_split.speech.str.split().apply(lambda x: ' '.join(x[2*int(len(x)/10):3*int(len(x)/10)]))\n",
    "sent_df_split['s4'] = sent_df_split.speech.str.split().apply(lambda x: ' '.join(x[3*int(len(x)/10):4*int(len(x)/10)]))\n",
    "sent_df_split['s5'] = sent_df_split.speech.str.split().apply(lambda x: ' '.join(x[4*int(len(x)/10):5*int(len(x)/10)]))\n",
    "sent_df_split['s6'] = sent_df_split.speech.str.split().apply(lambda x: ' '.join(x[5*int(len(x)/10):6*int(len(x)/10)]))\n",
    "sent_df_split['s7'] = sent_df_split.speech.str.split().apply(lambda x: ' '.join(x[6*int(len(x)/10):7*int(len(x)/10)]))\n",
    "sent_df_split['s8'] = sent_df_split.speech.str.split().apply(lambda x: ' '.join(x[7*int(len(x)/10):8*int(len(x)/10)]))\n",
    "sent_df_split['s9'] = sent_df_split.speech.str.split().apply(lambda x: ' '.join(x[8*int(len(x)/10):9*int(len(x)/10)]))\n",
    "sent_df_split['s10'] = sent_df_split.speech.str.split().apply(lambda x: ' '.join(x[9*int(len(x)/10):10*int(len(x)/10)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def obtain_comp_score(data, col):\n",
    "    \"\"\"Takes in a dataframe and column and return the compound sentiment score for the text\n",
    "    in that column. Used to obtain sentiment scores for the 10 smaller sections of the \n",
    "    commencement speecches.\"\"\"\n",
    "    \n",
    "    new_col_name = 'comp' + col\n",
    "    data[new_col_name] = data[col].apply(lambda x: analyser.polarity_scores(x)['compound'])\n",
    "    return data\n",
    "\n",
    "sent_df_split = obtain_comp_score(sent_df_split, 's1')\n",
    "sent_df_split = obtain_comp_score(sent_df_split, 's2')\n",
    "sent_df_split = obtain_comp_score(sent_df_split, 's3')\n",
    "sent_df_split = obtain_comp_score(sent_df_split, 's4')\n",
    "sent_df_split = obtain_comp_score(sent_df_split, 's5')\n",
    "sent_df_split = obtain_comp_score(sent_df_split, 's6')\n",
    "sent_df_split = obtain_comp_score(sent_df_split, 's7')\n",
    "sent_df_split = obtain_comp_score(sent_df_split, 's8')\n",
    "sent_df_split = obtain_comp_score(sent_df_split, 's9')\n",
    "sent_df_split = obtain_comp_score(sent_df_split, 's10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# view the average sentiment for each section of the speeches by gender\n",
    "sent_by_gender = sent_df_split.groupby('gender')['comps1',\n",
    "                                                'comps2',\n",
    "                                                'comps3',\n",
    "                                                'comps4',\n",
    "                                                'comps5',\n",
    "                                                'comps6',\n",
    "                                                'comps7',\n",
    "                                                'comps8',\n",
    "                                                'comps9',\n",
    "                                                'comps10'].agg(['mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create a dataframe of mean sentiment by gender to export for Tableau visualization\n",
    "sent_each = pd.DataFrame({'f':list(sent_by_gender.iloc[0,:].values) , \n",
    "                          'm': list(sent_by_gender.iloc[1,:].values)})\n",
    "\n",
    "# export for vis\n",
    "sent_each.to_csv('mean_sentiment_per_gender.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# view the overall top words per section to have an idea of where sentiment change is derived from\n",
    "\n",
    "def top_words_by_section(data, section):\n",
    "    \"\"\"Takes in a dataframe and a section and returns the most commonly used words within\n",
    "    that section.\"\"\"\n",
    "    cv = CountVectorizer(stop_words=create_stopwords(), ngram_range=(1,1))\n",
    "    data_cv = cv.fit_transform(data[section])\n",
    "    \n",
    "    data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "    data_dtm.index = data.index\n",
    "    \n",
    "    top_words_ls = np.sum(data_dtm, axis=0).sort_values(ascending=False)\n",
    "\n",
    "    return top_words_ls\n",
    "\n",
    "# view top words in section 1\n",
    "print(\"Top words in section 1:'\\n'\", top_words_by_section(sent_df_split, 's1')[:20], '\\n')\n",
    "\n",
    "# view top words in a middle section\n",
    "print(\"Top words in section 5:'\\n'\", top_words_by_section(sent_df_split, 's5')[:20], '\\n')\n",
    "\n",
    "# view top words in a section 10\n",
    "print(\"Top words in section 10:'\\n'\", top_words_by_section(sent_df_split, 's10')[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## IBM Watson Tone Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# IBM_API_KEY = os.environ['IBM_API_KEY']\n",
    "\n",
    "from ibm_connect import ibm_connnection()\n",
    "\n",
    "# connect to ibm with authentication key stored in separate .py file (ibm_comment)\n",
    "imb_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# re-pull dataset from MongoDB because IBM's Tone Analyzer leverages sentence structure and \n",
    "# periods were stripped away during cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ibm_df = pull_from_mongo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def clean_text_ibm(text):\n",
    "    \"\"\"Remove text in square brackets and remove words containing numbers.\"\"\"\n",
    "\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = re.sub('[‘’“”…:\"\\\"><]', '', text)\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    return text\n",
    "\n",
    "ibm_round = lambda x: clean_text_for_ibm(x)\n",
    "ibm_df.speech = speeches.speech.apply(ibm_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# repeat other data touch ups\n",
    "ibm_df = ibm_df.drop('_id', axis=1)\n",
    "\n",
    "spanish_speeker1, spanish_speeker2 = 'Henry A. Wallace', 'Billy Collins'\n",
    "ibm_df = ibm_df[ibm_df.name != spanish_speeker1]\n",
    "ibm_df = ibm_df[ibm_df.name != spanish_speeker2]\n",
    "\n",
    "ibm_df = ibm_df.drop_duplicates(keep='first')\n",
    "\n",
    "# add gender column back in\n",
    "m_f_designation = pickle.load( open( \"m_f_designation.pkl\", \"rb\" ) )\n",
    "gender = re.sub('\\n', ' ', m_f_designation).split(' ')\n",
    "ibm_df[\"gender\"] = gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# due to YouTube transcripts, many of the speeches do not include periods. To isolate those\n",
    "# that do, create a column that indicates the number of sentences per speech.\n",
    "ibm_df['sent_len'] = ibm_df.speech.apply(lambda x: len(sent_tokenize(x)))\n",
    "\n",
    "# select only those with a reasonable amount of sentences\n",
    "ibm_df_use = ibm_df[ibm_df.sent_len >= 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create separate dataframes for male and female to run through Watson as gender information\n",
    "# will otherwise be lost\n",
    "f_for_ibm = ibm_df_use[ibm_df_use.gender =='1']\n",
    "m_for_ibm = ibm_df_use[ibm_df_use.gender =='0']\n",
    "\n",
    "# run watson for female speeches and store in result list\n",
    "resp_f_ls = []\n",
    "\n",
    "for s in f_for_ibm.speech:\n",
    "    resp = tone_analyzer.tone(\n",
    "        {'text': s},\n",
    "        content_type='application/json',\n",
    "        sentences=False\n",
    "    )\n",
    "    \n",
    "    resp_f_ls.append(resp.result)\n",
    "\n",
    "# run watson for male speeches and store in result list\n",
    "resp_m_ls = []\n",
    "\n",
    "for s in m_for_ibm.speech:\n",
    "    resp = tone_analyzer.tone(\n",
    "        {'text': s},\n",
    "        content_type='application/json',\n",
    "        sentences=False\n",
    "    )\n",
    "    \n",
    "    resp_m_ls.append(resp.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Watson output is a dictionary of dictionaries. Access information on tone and store \n",
    "# as a new dataframe for easier accessing\n",
    "\n",
    "# create female df & and add gender indicator column\n",
    "start_f_df = pd.DataFrame.from_dict(resp_f_ls[0]['document_tone']['tones'])\n",
    "for i in range(1, len(resp_f_ls)):\n",
    "    start_f_df = pd.concat((start_f_df, pd.DataFrame.from_dict(resp_f_ls[i]['document_tone']['tones'])), axis=0)\n",
    "\n",
    "start_f_df['gender'] = '1'\n",
    "\n",
    "# create male df and add gender indicator column\n",
    "start_m_df = pd.DataFrame.from_dict(resp_m_ls[0]['document_tone']['tones'])\n",
    "for i in range(len(resp_m_ls)):\n",
    "    start_m_df = pd.concat((start_m_df, pd.DataFrame.from_dict(resp_m_ls[i]['document_tone']['tones'])), axis=0)\n",
    "\n",
    "start_m_df['gender'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# concatenate the male and female dataframes and export to csv for Tableau visualization\n",
    "full_sent_df = pd.concat((start_f_df, start_m_df), axis=0)\n",
    "\n",
    "full_sent_df.to_csv('full_sent_df.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "264.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
